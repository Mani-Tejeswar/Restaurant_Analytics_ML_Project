{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xaa4iul2RFWm",
        "outputId": "3df7f334-a5da-413d-a2db-f760f2ea9695"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- CELL 1: Data Preprocessing for Recommendation System ---\n",
            "--- Step 1: Loading the Dataset ---\n",
            "Dataset loaded successfully!\n",
            "Initial dataset shape: (9551, 21)\n",
            "Initial 5 rows:\n",
            "| Restaurant ID   | Restaurant Name        | Country Code   | City             | Address                                                                 | Locality                                   | Locality Verbose                                             | Longitude   | Latitude   | Cuisines                         | Average Cost for two   | Currency         | Has Table booking   | Has Online delivery   | Is delivering now   | Switch to order menu   | Price range   | Aggregate rating   | Rating color   | Rating text   | Votes   |\n",
            "|:----------------|:-----------------------|:---------------|:-----------------|:------------------------------------------------------------------------|:-------------------------------------------|:-------------------------------------------------------------|:------------|:-----------|:---------------------------------|:-----------------------|:-----------------|:--------------------|:----------------------|:--------------------|:-----------------------|:--------------|:-------------------|:---------------|:--------------|:--------|\n",
            "| 6317637         | Le Petit Souffle       | 162            | Makati City      | Third Floor, Century City Mall, Kalayaan Avenue, Poblacion, Makati City | Century City Mall, Poblacion, Makati City  | Century City Mall, Poblacion, Makati City, Makati City       | 121.028     | 14.5654    | French, Japanese, Desserts       | 1100                   | Botswana Pula(P) | Yes                 | No                    | No                  | No                     | 3             | 4.8                | Dark Green     | Excellent     | 314     |\n",
            "| 6304287         | Izakaya Kikufuji       | 162            | Makati City      | Little Tokyo, 2277 Chino Roces Avenue, Legaspi Village, Makati City     | Little Tokyo, Legaspi Village, Makati City | Little Tokyo, Legaspi Village, Makati City, Makati City      | 121.014     | 14.5537    | Japanese                         | 1200                   | Botswana Pula(P) | Yes                 | No                    | No                  | No                     | 3             | 4.5                | Dark Green     | Excellent     | 591     |\n",
            "| 6300002         | Heat - Edsa Shangri-La | 162            | Mandaluyong City | Edsa Shangri-La, 1 Garden Way, Ortigas, Mandaluyong City                | Edsa Shangri-La, Ortigas, Mandaluyong City | Edsa Shangri-La, Ortigas, Mandaluyong City, Mandaluyong City | 121.057     | 14.5814    | Seafood, Asian, Filipino, Indian | 4000                   | Botswana Pula(P) | Yes                 | No                    | No                  | No                     | 4             | 4.4                | Green          | Very Good     | 270     |\n",
            "| 6318506         | Ooma                   | 162            | Mandaluyong City | Third Floor, Mega Fashion Hall, SM Megamall, Ortigas, Mandaluyong City  | SM Megamall, Ortigas, Mandaluyong City     | SM Megamall, Ortigas, Mandaluyong City, Mandaluyong City     | 121.056     | 14.5853    | Japanese, Sushi                  | 1500                   | Botswana Pula(P) | No                  | No                    | No                  | No                     | 4             | 4.9                | Dark Green     | Excellent     | 365     |\n",
            "| 6314302         | Sambo Kojin            | 162            | Mandaluyong City | Third Floor, Mega Atrium, SM Megamall, Ortigas, Mandaluyong City        | SM Megamall, Ortigas, Mandaluyong City     | SM Megamall, Ortigas, Mandaluyong City, Mandaluyong City     | 121.058     | 14.5845    | Japanese, Korean                 | 1500                   | Botswana Pula(P) | Yes                 | No                    | No                  | No                     | 4             | 4.8                | Dark Green     | Excellent     | 229     |\n",
            "\n",
            "--- Step 2: Handling Missing Values ---\n",
            "Missing values before handling:\n",
            "|          | 0   |\n",
            "|:---------|:----|\n",
            "| Cuisines | 9   |\n",
            "Dataset shape after dropping rows with missing 'Cuisines': (9542, 21)\n",
            "Missing values after handling:\n",
            "| 0   |\n",
            "|-----|\n",
            "\n",
            "--- Step 3: Encoding Categorical Variables ---\n",
            "Binary 'Yes'/'No' columns converted to 1/0.\n",
            "| Has Table booking   | Has Online delivery   | Is delivering now   | Switch to order menu   |\n",
            "|:--------------------|:----------------------|:--------------------|:-----------------------|\n",
            "| 1                   | 0                     | 0                   | 0                      |\n",
            "| 1                   | 0                     | 0                   | 0                      |\n",
            "| 1                   | 0                     | 0                   | 0                      |\n",
            "| 0                   | 0                     | 0                   | 0                      |\n",
            "| 1                   | 0                     | 0                   | 0                      |\n",
            "Dropped less relevant/redundant columns. Current shape: (9542, 15)\n",
            "Nominal categorical columns one-hot encoded. Current shape: (9542, 176)\n",
            "Explicitly dropped original categorical columns after one-hot encoding.\n",
            "'Cuisines' column multi-label encoded. Current shape: (9542, 320)\n",
            "\n",
            "--- Step 5: Feature Scaling ---\n",
            "Numerical features scaled using StandardScaler.\n",
            "\n",
            "Preprocessing for Recommendation System complete!\n",
            "Final processed DataFrame shape: (9542, 320)\n",
            "First 5 rows of processed data (features and IDs):\n",
            "| Restaurant ID   | Restaurant Name        | Longitude   | Latitude   | Average Cost for two   | Has Table booking   | Has Online delivery   | Is delivering now   | Price range   | Aggregate rating   | Votes    | Country Code_14   | Country Code_30   | Country Code_37   | Country Code_94   | Country Code_148   | Country Code_162   | Country Code_166   | Country Code_184   | Country Code_189   | Country Code_191   | Country Code_208   | Country Code_214   | Country Code_215   | Country Code_216   | City_Agra   | City_Ahmedabad   | City_Albany   | City_Allahabad   | City_Amritsar   | City_Ankara   | City_Armidale   | City_Athens   | City_Auckland   | City_Augusta   | City_Aurangabad   | City_Balingup   | City_Bandung   | City_Bangalore   | City_Beechworth   | City_Bhopal   | City_Bhubaneshwar   | City_Birmingham   | City_Bogor   | City_Boise   | City_Bras�_lia   | City_Cape Town   | City_Cedar Rapids/Iowa City   | City_Chandigarh   | City_Chatham-Kent   | City_Chennai   | City_Clatskanie   | City_Cochrane   | City_Coimbatore   | City_Colombo   | City_Columbus   | City_Consort   | City_Dalton   | City_Davenport   | City_Dehradun   | City_Des Moines   | City_Dicky Beach   | City_Doha   | City_Dubai   | City_Dubuque   | City_East Ballina   | City_Edinburgh   | City_Faridabad   | City_Fernley   | City_Flaxton   | City_Forrest   | City_Gainesville   | City_Ghaziabad   | City_Goa   | City_Gurgaon   | City_Guwahati   | City_Hepburn Springs   | City_Huskisson   | City_Hyderabad   | City_Indore   | City_Inner City   | City_Inverloch   | City_Jaipur   | City_Jakarta   | City_Johannesburg   | City_Kanpur   | City_Kochi   | City_Kolkata   | City_Lakes Entrance   | City_Lakeview   | City_Lincoln   | City_London   | City_Lorn   | City_Lucknow   | City_Ludhiana   | City_Macedon   | City_Macon   | City_Makati City   | City_Manchester   | City_Mandaluyong City   | City_Mangalore   | City_Mayfield   | City_Mc Millan   | City_Middleton Beach   | City_Mohali   | City_Monroe   | City_Montville   | City_Mumbai   | City_Mysore   | City_Nagpur   | City_Nashik   | City_New Delhi   | City_Noida   | City_Ojo Caliente   | City_Orlando   | City_Palm Cove   | City_Panchkula   | City_Pasay City   | City_Pasig City   | City_Patna   | City_Paynesville   | City_Penola   | City_Pensacola   | City_Phillip Island   | City_Pocatello   | City_Potrero   | City_Pretoria   | City_Princeton   | City_Puducherry   | City_Pune   | City_Quezon City   | City_Ranchi   | City_Randburg   | City_Rest of Hawaii   | City_Rio de Janeiro   | City_San Juan City   | City_Sandton   | City_Santa Rosa   | City_Savannah   | City_Secunderabad   | City_Sharjah   | City_Singapore   | City_Sioux City   | City_Surat   | City_S��o Paulo   | City_Tagaytay City   | City_Taguig City   | City_Tampa Bay   | City_Tangerang   | City_Tanunda   | City_Trentham East   | City_Vadodara   | City_Valdosta   | City_Varanasi   | City_Vernonia   | City_Victor Harbor   | City_Vineland Station   | City_Vizag   | City_Waterloo   | City_Weirton   | City_Wellington City   | City_Winchester Bay   | City_Yorkton   | City_��stanbul   | Currency_Brazilian Real(R$)   | Currency_Dollar($)   | Currency_Emirati Diram(AED)   | Currency_Indian Rupees(Rs.)   | Currency_Indonesian Rupiah(IDR)   | Currency_NewZealand($)   | Currency_Pounds(��)   | Currency_Qatari Rial(QR)   | Currency_Rand(R)   | Currency_Sri Lankan Rupee(LKR)   | Currency_Turkish Lira(TL)   | Afghani   | African   | American   | Andhra   | Arabian   | Argentine   | Armenian   | Asian   | Asian Fusion   | Assamese   | Australian   | Awadhi   | BBQ   | Bakery   | Bar Food   | Belgian   | Bengali   | Beverages   | Bihari   | Biryani   | Brazilian   | Breakfast   | British   | Bubble Tea   | Burger   | Burmese   | B�_rek   | Cafe   | Cajun   | Canadian   | Cantonese   | Caribbean   | Charcoal Grill   | Chettinad   | Chinese   | Coffee and Tea   | Contemporary   | Continental   | Cuban   | Cuisine Varies   | Curry   | Deli   | Desserts   | Dim Sum   | Diner   | Drinks Only   | Durban   | D�_ner   | European   | Fast Food   | Filipino   | Finger Food   | Fish and Chips   | French   | Fusion   | German   | Goan   | Gourmet Fast Food   | Greek   | Grill   | Gujarati   | Hawaiian   | Healthy Food   | Hyderabadi   | Ice Cream   | Indian   | Indonesian   | International   | Iranian   | Irish   | Italian   | Izgara   | Japanese   | Juices   | Kashmiri   | Kebab   | Kerala   | Kiwi   | Korean   | Latin American   | Lebanese   | Lucknowi   | Maharashtrian   | Malay   | Malaysian   | Malwani   | Mangalorean   | Mediterranean   | Mexican   | Middle Eastern   | Mineira   | Mithai   | Modern Australian   | Modern Indian   | Moroccan   | Mughlai   | Naga   | Nepalese   | New American   | North Eastern   | North Indian   | Oriya   | Pakistani   | Parsi   | Patisserie   | Peranakan   | Persian   | Peruvian   | Pizza   | Portuguese   | Pub Food   | Rajasthani   | Ramen   | Raw Meats   | Restaurant Cafe   | Salad   | Sandwich   | Scottish   | Seafood   | Singaporean   | Soul Food   | South African   | South American   | South Indian   | Southern   | Southwestern   | Spanish   | Sri Lankan   | Steak   | Street Food   | Sunda   | Sushi   | Taiwanese   | Tapas   | Tea   | Teriyaki   | Tex-Mex   | Thai   | Tibetan   | Turkish   | Turkish Pizza   | Vegetarian   | Vietnamese   | Western   | World Cuisine   |\n",
            "|:----------------|:-----------------------|:------------|:-----------|:-----------------------|:--------------------|:----------------------|:--------------------|:--------------|:-------------------|:---------|:------------------|:------------------|:------------------|:------------------|:-------------------|:-------------------|:-------------------|:-------------------|:-------------------|:-------------------|:-------------------|:-------------------|:-------------------|:-------------------|:------------|:-----------------|:--------------|:-----------------|:----------------|:--------------|:----------------|:--------------|:----------------|:---------------|:------------------|:----------------|:---------------|:-----------------|:------------------|:--------------|:--------------------|:------------------|:-------------|:-------------|:-----------------|:-----------------|:------------------------------|:------------------|:--------------------|:---------------|:------------------|:----------------|:------------------|:---------------|:----------------|:---------------|:--------------|:-----------------|:----------------|:------------------|:-------------------|:------------|:-------------|:---------------|:--------------------|:-----------------|:-----------------|:---------------|:---------------|:---------------|:-------------------|:-----------------|:-----------|:---------------|:----------------|:-----------------------|:-----------------|:-----------------|:--------------|:------------------|:-----------------|:--------------|:---------------|:--------------------|:--------------|:-------------|:---------------|:----------------------|:----------------|:---------------|:--------------|:------------|:---------------|:----------------|:---------------|:-------------|:-------------------|:------------------|:------------------------|:-----------------|:----------------|:-----------------|:-----------------------|:--------------|:--------------|:-----------------|:--------------|:--------------|:--------------|:--------------|:-----------------|:-------------|:--------------------|:---------------|:-----------------|:-----------------|:------------------|:------------------|:-------------|:-------------------|:--------------|:-----------------|:----------------------|:-----------------|:---------------|:----------------|:-----------------|:------------------|:------------|:-------------------|:--------------|:----------------|:----------------------|:----------------------|:---------------------|:---------------|:------------------|:----------------|:--------------------|:---------------|:-----------------|:------------------|:-------------|:------------------|:---------------------|:-------------------|:-----------------|:-----------------|:---------------|:---------------------|:----------------|:----------------|:----------------|:----------------|:---------------------|:------------------------|:-------------|:----------------|:---------------|:-----------------------|:----------------------|:---------------|:-----------------|:------------------------------|:---------------------|:------------------------------|:------------------------------|:----------------------------------|:-------------------------|:----------------------|:---------------------------|:-------------------|:---------------------------------|:----------------------------|:----------|:----------|:-----------|:---------|:----------|:------------|:-----------|:--------|:---------------|:-----------|:-------------|:---------|:------|:---------|:-----------|:----------|:----------|:------------|:---------|:----------|:------------|:------------|:----------|:-------------|:---------|:----------|:---------|:-------|:--------|:-----------|:------------|:------------|:-----------------|:------------|:----------|:-----------------|:---------------|:--------------|:--------|:-----------------|:--------|:-------|:-----------|:----------|:--------|:--------------|:---------|:---------|:-----------|:------------|:-----------|:--------------|:-----------------|:---------|:---------|:---------|:-------|:--------------------|:--------|:--------|:-----------|:-----------|:---------------|:-------------|:------------|:---------|:-------------|:----------------|:----------|:--------|:----------|:---------|:-----------|:---------|:-----------|:--------|:---------|:-------|:---------|:-----------------|:-----------|:-----------|:----------------|:--------|:------------|:----------|:--------------|:----------------|:----------|:-----------------|:----------|:---------|:--------------------|:----------------|:-----------|:----------|:-------|:-----------|:---------------|:----------------|:---------------|:--------|:------------|:--------|:-------------|:------------|:----------|:-----------|:--------|:-------------|:-----------|:-------------|:--------|:------------|:------------------|:--------|:-----------|:-----------|:----------|:--------------|:------------|:----------------|:-----------------|:---------------|:-----------|:---------------|:----------|:-------------|:--------|:--------------|:--------|:--------|:------------|:--------|:------|:-----------|:----------|:-------|:----------|:----------|:----------------|:-------------|:-------------|:----------|:----------------|\n",
            "| 6317637         | Le Petit Souffle       | 1.37764     | -1.02485   | -0.00622066            | 2.69074             | -0.58792              | -0.0597991          | 1.31973       | 4.8                | 0.365493 | False             | False             | False             | False             | False              | True               | False              | False              | False              | False              | False              | False              | False              | False              | False       | False            | False         | False            | False           | False         | False           | False         | False           | False          | False             | False           | False          | False            | False             | False         | False               | False             | False        | False        | False            | False            | False                         | False             | False               | False          | False             | False           | False             | False          | False           | False          | False         | False            | False           | False             | False              | False       | False        | False          | False               | False            | False            | False          | False          | False          | False              | False            | False      | False          | False           | False                  | False            | False            | False         | False             | False            | False         | False          | False               | False         | False        | False          | False                 | False           | False          | False         | False       | False          | False           | False          | False        | True               | False             | False                   | False            | False           | False            | False                  | False         | False         | False            | False         | False         | False         | False         | False            | False        | False               | False          | False            | False            | False             | False             | False        | False              | False         | False            | False                 | False            | False          | False           | False            | False             | False       | False              | False         | False           | False                 | False                 | False                | False          | False             | False           | False               | False          | False            | False             | False        | False             | False                | False              | False            | False            | False          | False                | False           | False           | False           | False           | False                | False                   | False        | False           | False          | False                  | False                 | False          | False            | False                         | False                | False                         | False                         | False                             | False                    | False                 | False                      | False              | False                            | False                       | 0         | 0         | 0          | 0        | 0         | 0           | 0          | 0       | 0              | 0          | 0            | 0        | 0     | 0        | 0          | 0         | 0         | 0           | 0        | 0         | 0           | 0           | 0         | 0            | 0        | 0         | 0        | 0      | 0       | 0          | 0           | 0           | 0                | 0           | 0         | 0                | 0              | 0             | 0       | 0                | 0       | 0      | 1          | 0         | 0       | 0             | 0        | 0        | 0          | 0           | 0          | 0             | 0                | 1        | 0        | 0        | 0      | 0                   | 0       | 0       | 0          | 0          | 0              | 0            | 0           | 0        | 0            | 0               | 0         | 0       | 0         | 0        | 1          | 0        | 0          | 0       | 0        | 0      | 0        | 0                | 0          | 0          | 0               | 0       | 0           | 0         | 0             | 0               | 0         | 0                | 0         | 0        | 0                   | 0               | 0          | 0         | 0      | 0          | 0              | 0               | 0              | 0       | 0           | 0       | 0            | 0           | 0         | 0          | 0       | 0            | 0          | 0            | 0       | 0           | 0                 | 0       | 0          | 0          | 0         | 0             | 0           | 0               | 0                | 0              | 0          | 0              | 0         | 0            | 0       | 0             | 0       | 0       | 0           | 0       | 0     | 0          | 0         | 0      | 0         | 0         | 0               | 0            | 0            | 0         | 0               |\n",
            "| 6304287         | Izakaya Kikufuji       | 1.37732     | -1.02591   | -2.02219e-05           | 2.69074             | -0.58792              | -0.0597991          | 1.31973       | 4.5                | 1.00941  | False             | False             | False             | False             | False              | True               | False              | False              | False              | False              | False              | False              | False              | False              | False       | False            | False         | False            | False           | False         | False           | False         | False           | False          | False             | False           | False          | False            | False             | False         | False               | False             | False        | False        | False            | False            | False                         | False             | False               | False          | False             | False           | False             | False          | False           | False          | False         | False            | False           | False             | False              | False       | False        | False          | False               | False            | False            | False          | False          | False          | False              | False            | False      | False          | False           | False                  | False            | False            | False         | False             | False            | False         | False          | False               | False         | False        | False          | False                 | False           | False          | False         | False       | False          | False           | False          | False        | True               | False             | False                   | False            | False           | False            | False                  | False         | False         | False            | False         | False         | False         | False         | False            | False        | False               | False          | False            | False            | False             | False             | False        | False              | False         | False            | False                 | False            | False          | False           | False            | False             | False       | False              | False         | False           | False                 | False                 | False                | False          | False             | False           | False               | False          | False            | False             | False        | False             | False                | False              | False            | False            | False          | False                | False           | False           | False           | False           | False                | False                   | False        | False           | False          | False                  | False                 | False          | False            | False                         | False                | False                         | False                         | False                             | False                    | False                 | False                      | False              | False                            | False                       | 0         | 0         | 0          | 0        | 0         | 0           | 0          | 0       | 0              | 0          | 0            | 0        | 0     | 0        | 0          | 0         | 0         | 0           | 0        | 0         | 0           | 0           | 0         | 0            | 0        | 0         | 0        | 0      | 0       | 0          | 0           | 0           | 0                | 0           | 0         | 0                | 0              | 0             | 0       | 0                | 0       | 0      | 0          | 0         | 0       | 0             | 0        | 0        | 0          | 0           | 0          | 0             | 0                | 0        | 0        | 0        | 0      | 0                   | 0       | 0       | 0          | 0          | 0              | 0            | 0           | 0        | 0            | 0               | 0         | 0       | 0         | 0        | 1          | 0        | 0          | 0       | 0        | 0      | 0        | 0                | 0          | 0          | 0               | 0       | 0           | 0         | 0             | 0               | 0         | 0                | 0         | 0        | 0                   | 0               | 0          | 0         | 0      | 0          | 0              | 0               | 0              | 0       | 0           | 0       | 0            | 0           | 0         | 0          | 0       | 0            | 0          | 0            | 0       | 0           | 0                 | 0       | 0          | 0          | 0         | 0             | 0           | 0               | 0                | 0              | 0          | 0              | 0         | 0            | 0       | 0             | 0       | 0       | 0           | 0       | 0     | 0          | 0         | 0      | 0         | 0         | 0               | 0            | 0            | 0         | 0               |\n",
            "| 6300002         | Heat - Edsa Shangri-La | 1.37835     | -1.0234    | 0.173592               | 2.69074             | -0.58792              | -0.0597991          | 2.42407       | 4.4                | 0.26321  | False             | False             | False             | False             | False              | True               | False              | False              | False              | False              | False              | False              | False              | False              | False       | False            | False         | False            | False           | False         | False           | False         | False           | False          | False             | False           | False          | False            | False             | False         | False               | False             | False        | False        | False            | False            | False                         | False             | False               | False          | False             | False           | False             | False          | False           | False          | False         | False            | False           | False             | False              | False       | False        | False          | False               | False            | False            | False          | False          | False          | False              | False            | False      | False          | False           | False                  | False            | False            | False         | False             | False            | False         | False          | False               | False         | False        | False          | False                 | False           | False          | False         | False       | False          | False           | False          | False        | False              | False             | True                    | False            | False           | False            | False                  | False         | False         | False            | False         | False         | False         | False         | False            | False        | False               | False          | False            | False            | False             | False             | False        | False              | False         | False            | False                 | False            | False          | False           | False            | False             | False       | False              | False         | False           | False                 | False                 | False                | False          | False             | False           | False               | False          | False            | False             | False        | False             | False                | False              | False            | False            | False          | False                | False           | False           | False           | False           | False                | False                   | False        | False           | False          | False                  | False                 | False          | False            | False                         | False                | False                         | False                         | False                             | False                    | False                 | False                      | False              | False                            | False                       | 0         | 0         | 0          | 0        | 0         | 0           | 0          | 1       | 0              | 0          | 0            | 0        | 0     | 0        | 0          | 0         | 0         | 0           | 0        | 0         | 0           | 0           | 0         | 0            | 0        | 0         | 0        | 0      | 0       | 0          | 0           | 0           | 0                | 0           | 0         | 0                | 0              | 0             | 0       | 0                | 0       | 0      | 0          | 0         | 0       | 0             | 0        | 0        | 0          | 0           | 1          | 0             | 0                | 0        | 0        | 0        | 0      | 0                   | 0       | 0       | 0          | 0          | 0              | 0            | 0           | 1        | 0            | 0               | 0         | 0       | 0         | 0        | 0          | 0        | 0          | 0       | 0        | 0      | 0        | 0                | 0          | 0          | 0               | 0       | 0           | 0         | 0             | 0               | 0         | 0                | 0         | 0        | 0                   | 0               | 0          | 0         | 0      | 0          | 0              | 0               | 0              | 0       | 0           | 0       | 0            | 0           | 0         | 0          | 0       | 0            | 0          | 0            | 0       | 0           | 0                 | 0       | 0          | 0          | 1         | 0             | 0           | 0               | 0                | 0              | 0          | 0              | 0         | 0            | 0       | 0             | 0       | 0       | 0           | 0       | 0     | 0          | 0         | 0      | 0         | 0         | 0               | 0            | 0            | 0         | 0               |\n",
            "| 6318506         | Ooma                   | 1.37834     | -1.02304   | 0.0185811              | -0.371645           | -0.58792              | -0.0597991          | 2.42407       | 4.9                | 0.484048 | False             | False             | False             | False             | False              | True               | False              | False              | False              | False              | False              | False              | False              | False              | False       | False            | False         | False            | False           | False         | False           | False         | False           | False          | False             | False           | False          | False            | False             | False         | False               | False             | False        | False        | False            | False            | False                         | False             | False               | False          | False             | False           | False             | False          | False           | False          | False         | False            | False           | False             | False              | False       | False        | False          | False               | False            | False            | False          | False          | False          | False              | False            | False      | False          | False           | False                  | False            | False            | False         | False             | False            | False         | False          | False               | False         | False        | False          | False                 | False           | False          | False         | False       | False          | False           | False          | False        | False              | False             | True                    | False            | False           | False            | False                  | False         | False         | False            | False         | False         | False         | False         | False            | False        | False               | False          | False            | False            | False             | False             | False        | False              | False         | False            | False                 | False            | False          | False           | False            | False             | False       | False              | False         | False           | False                 | False                 | False                | False          | False             | False           | False               | False          | False            | False             | False        | False             | False                | False              | False            | False            | False          | False                | False           | False           | False           | False           | False                | False                   | False        | False           | False          | False                  | False                 | False          | False            | False                         | False                | False                         | False                         | False                             | False                    | False                 | False                      | False              | False                            | False                       | 0         | 0         | 0          | 0        | 0         | 0           | 0          | 0       | 0              | 0          | 0            | 0        | 0     | 0        | 0          | 0         | 0         | 0           | 0        | 0         | 0           | 0           | 0         | 0            | 0        | 0         | 0        | 0      | 0       | 0          | 0           | 0           | 0                | 0           | 0         | 0                | 0              | 0             | 0       | 0                | 0       | 0      | 0          | 0         | 0       | 0             | 0        | 0        | 0          | 0           | 0          | 0             | 0                | 0        | 0        | 0        | 0      | 0                   | 0       | 0       | 0          | 0          | 0              | 0            | 0           | 0        | 0            | 0               | 0         | 0       | 0         | 0        | 1          | 0        | 0          | 0       | 0        | 0      | 0        | 0                | 0          | 0          | 0               | 0       | 0           | 0         | 0             | 0               | 0         | 0                | 0         | 0        | 0                   | 0               | 0          | 0         | 0      | 0          | 0              | 0               | 0              | 0       | 0           | 0       | 0            | 0           | 0         | 0          | 0       | 0            | 0          | 0            | 0       | 0           | 0                 | 0       | 0          | 0          | 0         | 0             | 0           | 0               | 0                | 0              | 0          | 0              | 0         | 0            | 0       | 0             | 0       | 1       | 0           | 0       | 0     | 0          | 0         | 0      | 0         | 0         | 0               | 0            | 0            | 0         | 0               |\n",
            "| 6314302         | Sambo Kojin            | 1.37837     | -1.02312   | 0.0185811              | 2.69074             | -0.58792              | -0.0597991          | 2.42407       | 4.8                | 0.167901 | False             | False             | False             | False             | False              | True               | False              | False              | False              | False              | False              | False              | False              | False              | False       | False            | False         | False            | False           | False         | False           | False         | False           | False          | False             | False           | False          | False            | False             | False         | False               | False             | False        | False        | False            | False            | False                         | False             | False               | False          | False             | False           | False             | False          | False           | False          | False         | False            | False           | False             | False              | False       | False        | False          | False               | False            | False            | False          | False          | False          | False              | False            | False      | False          | False           | False                  | False            | False            | False         | False             | False            | False         | False          | False               | False         | False        | False          | False                 | False           | False          | False         | False       | False          | False           | False          | False        | False              | False             | True                    | False            | False           | False            | False                  | False         | False         | False            | False         | False         | False         | False         | False            | False        | False               | False          | False            | False            | False             | False             | False        | False              | False         | False            | False                 | False            | False          | False           | False            | False             | False       | False              | False         | False           | False                 | False                 | False                | False          | False             | False           | False               | False          | False            | False             | False        | False             | False                | False              | False            | False            | False          | False                | False           | False           | False           | False           | False                | False                   | False        | False           | False          | False                  | False                 | False          | False            | False                         | False                | False                         | False                         | False                             | False                    | False                 | False                      | False              | False                            | False                       | 0         | 0         | 0          | 0        | 0         | 0           | 0          | 0       | 0              | 0          | 0            | 0        | 0     | 0        | 0          | 0         | 0         | 0           | 0        | 0         | 0           | 0           | 0         | 0            | 0        | 0         | 0        | 0      | 0       | 0          | 0           | 0           | 0                | 0           | 0         | 0                | 0              | 0             | 0       | 0                | 0       | 0      | 0          | 0         | 0       | 0             | 0        | 0        | 0          | 0           | 0          | 0             | 0                | 0        | 0        | 0        | 0      | 0                   | 0       | 0       | 0          | 0          | 0              | 0            | 0           | 0        | 0            | 0               | 0         | 0       | 0         | 0        | 1          | 0        | 0          | 0       | 0        | 0      | 1        | 0                | 0          | 0          | 0               | 0       | 0           | 0         | 0             | 0               | 0         | 0                | 0         | 0        | 0                   | 0               | 0          | 0         | 0      | 0          | 0              | 0               | 0              | 0       | 0           | 0       | 0            | 0           | 0         | 0          | 0       | 0            | 0          | 0            | 0       | 0           | 0                 | 0       | 0          | 0          | 0         | 0             | 0           | 0               | 0                | 0              | 0          | 0              | 0         | 0            | 0       | 0             | 0       | 0       | 0           | 0       | 0     | 0          | 0         | 0      | 0         | 0         | 0               | 0            | 0            | 0         | 0               |\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import os\n",
        "\n",
        "# --- Global variables for preprocessing consistency in recommendation ---\n",
        "global scaler_recommender, all_features_cols_recommender\n",
        "global original_categorical_cols_recommender, original_binary_cols_recommender, original_all_cuisines_recommender\n",
        "global df_processed_for_recommender # The final processed DataFrame for recommendations\n",
        "\n",
        "print(\"--- CELL 1: Data Preprocessing for Recommendation System ---\")\n",
        "\n",
        "# --- 1. Load the Dataset ---\n",
        "print(\"--- Step 1: Loading the Dataset ---\")\n",
        "file_path = 'Dataset .csv'\n",
        "if not os.path.exists(file_path):\n",
        "    print(f\"Error: '{file_path}' not found. Please ensure the dataset file is in the same directory.\")\n",
        "    exit()\n",
        "\n",
        "df = pd.read_csv(file_path)\n",
        "print(\"Dataset loaded successfully!\")\n",
        "\n",
        "print(f\"Initial dataset shape: {df.shape}\")\n",
        "print(\"Initial 5 rows:\")\n",
        "print(df.head().to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
        "\n",
        "\n",
        "# --- 2. Handle Missing Values ---\n",
        "print(\"\\n--- Step 2: Handling Missing Values ---\")\n",
        "print(\"Missing values before handling:\")\n",
        "print(df.isnull().sum()[df.isnull().sum() > 0].to_markdown(numalign=\"left\", stralign=\"left\"))\n",
        "\n",
        "df.dropna(subset=['Cuisines'], inplace=True)\n",
        "print(f\"Dataset shape after dropping rows with missing 'Cuisines': {df.shape}\")\n",
        "print(\"Missing values after handling:\")\n",
        "print(df.isnull().sum()[df.isnull().sum() > 0].to_markdown(numalign=\"left\", stralign=\"left\"))\n",
        "\n",
        "\n",
        "# --- 3. Encode Categorical Variables ---\n",
        "print(\"\\n--- Step 3: Encoding Categorical Variables ---\")\n",
        "\n",
        "# Convert binary 'Yes'/'No' columns to 1/0\n",
        "original_binary_cols_recommender = ['Has Table booking', 'Has Online delivery', 'Is delivering now', 'Switch to order menu']\n",
        "for col in original_binary_cols_recommender:\n",
        "    df[col] = df[col].apply(lambda x: 1 if x == 'Yes' else 0)\n",
        "print(\"Binary 'Yes'/'No' columns converted to 1/0.\")\n",
        "print(df[original_binary_cols_recommender].head().to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
        "\n",
        "\n",
        "# Store all unique cuisines for consistent multi-label encoding\n",
        "original_all_cuisines_recommender = sorted(df['Cuisines'].str.split(', ').explode().unique())\n",
        "\n",
        "\n",
        "# Identify columns to drop (less relevant for content-based similarity or redundant)\n",
        "# We KEEP 'Restaurant ID', 'Restaurant Name', 'Aggregate rating' for recommendations\n",
        "columns_to_drop_recommender = [\n",
        "    'Address',\n",
        "    'Locality',\n",
        "    'Locality Verbose',\n",
        "    'Switch to order menu', # Only 'No' values, not useful\n",
        "    'Rating color',         # Highly correlated with target, less useful for independent content-based recs\n",
        "    'Rating text'           # Highly correlated with target\n",
        "]\n",
        "df.drop(columns=columns_to_drop_recommender, inplace=True)\n",
        "print(f\"Dropped less relevant/redundant columns. Current shape: {df.shape}\")\n",
        "\n",
        "\n",
        "# One-Hot Encode nominal categorical columns\n",
        "original_categorical_cols_recommender = ['Country Code', 'City', 'Currency']\n",
        "df = pd.get_dummies(df, columns=original_categorical_cols_recommender, drop_first=True)\n",
        "print(f\"Nominal categorical columns one-hot encoded. Current shape: {df.shape}\")\n",
        "\n",
        "# --- FIX: Explicitly drop the original columns after one-hot encoding ---\n",
        "# This ensures they are not present in df_processed_for_recommender\n",
        "df.drop(columns=[col for col in original_categorical_cols_recommender if col in df.columns], inplace=True, errors='ignore')\n",
        "print(\"Explicitly dropped original categorical columns after one-hot encoding.\")\n",
        "\n",
        "\n",
        "# Handle 'Cuisines' column with multi-label one-hot encoding\n",
        "cuisine_dummies = df['Cuisines'].str.get_dummies(sep=', ')\n",
        "df = pd.concat([df, cuisine_dummies], axis=1)\n",
        "df.drop(columns=['Cuisines'], inplace=True)\n",
        "print(f\"'Cuisines' column multi-label encoded. Current shape: {df.shape}\")\n",
        "\n",
        "\n",
        "# --- 4. Identify Features for Similarity Calculation and Scaling ---\n",
        "# Features that describe the restaurant's content/attributes\n",
        "# Exclude ID, Name, Rating as they are not features for similarity calculation\n",
        "features_for_similarity = [col for col in df.columns if col not in ['Restaurant ID', 'Restaurant Name', 'Aggregate rating']]\n",
        "numerical_cols_to_scale = ['Longitude', 'Latitude', 'Average Cost for two', 'Price range', 'Votes', 'Has Table booking', 'Has Online delivery', 'Is delivering now']\n",
        "# Ensure only numerical columns that are actually in the df are in the list to scale\n",
        "numerical_cols_to_scale = [col for col in numerical_cols_to_scale if col in df.columns and pd.api.types.is_numeric_dtype(df[col])]\n",
        "\n",
        "# Store all feature columns for consistent user profile creation\n",
        "# This will now correctly contain only the one-hot encoded and numerical features\n",
        "all_features_cols_recommender = df.drop(columns=['Restaurant ID', 'Restaurant Name', 'Aggregate rating'], errors='ignore').columns\n",
        "\n",
        "\n",
        "# --- 5. Feature Scaling ---\n",
        "print(\"\\n--- Step 5: Feature Scaling ---\")\n",
        "scaler_recommender = StandardScaler()\n",
        "# Fit and transform only the numerical features\n",
        "df[numerical_cols_to_scale] = scaler_recommender.fit_transform(df[numerical_cols_to_scale])\n",
        "print(\"Numerical features scaled using StandardScaler.\")\n",
        "\n",
        "# Final DataFrame for the recommender, including ID, Name, Rating, and all processed features\n",
        "df_processed_for_recommender = df.copy()\n",
        "\n",
        "print(\"\\nPreprocessing for Recommendation System complete!\")\n",
        "print(f\"Final processed DataFrame shape: {df_processed_for_recommender.shape}\")\n",
        "print(\"First 5 rows of processed data (features and IDs):\")\n",
        "print(df_processed_for_recommender.head().to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "print(\"--- CELL 2: Recommendation System Core Logic ---\")\n",
        "\n",
        "def create_user_profile(user_preferences, df_template, scaler_obj, ohe_cat_cols, bin_cols, all_cuisines_list, num_cols_to_scale):\n",
        "    \"\"\"\n",
        "    Creates a preprocessed user profile vector based on preferences,\n",
        "    matching the feature space of the restaurant data.\n",
        "\n",
        "    Args:\n",
        "        user_preferences (dict): Dictionary of user preferences.\n",
        "                                 Keys should match original column names.\n",
        "                                 'Cuisines' should be a list of strings.\n",
        "                                 'Min_Rating' is a filter, not a feature for similarity.\n",
        "        df_template (pd.DataFrame): A DataFrame with the same columns as the processed\n",
        "                                    restaurant features (e.g., X_train.columns or all_features_cols_recommender).\n",
        "                                    Used to ensure consistent column order and presence.\n",
        "        scaler_obj (StandardScaler): The fitted scaler used for numerical features.\n",
        "        ohe_cat_cols (list): List of original categorical columns for one-hot encoding.\n",
        "        bin_cols (list): List of original binary columns for 0/1 conversion.\n",
        "        all_cuisines_list (list): List of all unique cuisines for multi-label encoding.\n",
        "        num_cols_to_scale (list): List of numerical columns that were scaled.\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: A preprocessed user profile vector.\n",
        "    \"\"\"\n",
        "    # Create a DataFrame for the user's preferences\n",
        "    user_df = pd.DataFrame([user_preferences])\n",
        "\n",
        "    # 1. Handle binary columns\n",
        "    for col in bin_cols:\n",
        "        if col in user_df.columns:\n",
        "            user_df[col] = user_df[col].apply(lambda x: 1 if x == 'Yes' else 0)\n",
        "        else:\n",
        "            user_df[col] = 0 # Assume 'No' if not provided\n",
        "\n",
        "    # 2. Handle Cuisines (multi-label one-hot encoding)\n",
        "    cuisine_data = {cuisine: 0 for cuisine in all_cuisines_list}\n",
        "    if 'Cuisines' in user_df.columns and user_df['Cuisines'].iloc[0]:\n",
        "        # Ensure the cuisine list is not empty\n",
        "        cuisines_list = user_df['Cuisines'].iloc[0]\n",
        "        if isinstance(cuisines_list, list):\n",
        "             current_cuisines = [c.strip() for c in cuisines_list if c.strip()]\n",
        "             for cuisine in current_cuisines:\n",
        "                 if cuisine in cuisine_data:\n",
        "                     cuisine_data[cuisine] = 1\n",
        "    new_cuisine_df = pd.DataFrame([cuisine_data]) # Convert dict to DataFrame for concat\n",
        "\n",
        "\n",
        "    # Drop original 'Cuisines' from user_df if it exists\n",
        "    if 'Cuisines' in user_df.columns:\n",
        "        user_df.drop(columns=['Cuisines'], inplace=True)\n",
        "\n",
        "    # 3. One-Hot Encode other nominal categorical columns\n",
        "    # Identify which of the ohe_cat_cols are actually in the user_df\n",
        "    cols_to_ohe_in_user_df = [col for col in ohe_cat_cols if col in user_df.columns]\n",
        "    user_df_encoded = pd.get_dummies(user_df, columns=cols_to_ohe_in_user_df, drop_first=True)\n",
        "\n",
        "    # Concatenate cuisine dummies and other encoded features\n",
        "    user_profile_df = pd.concat([user_df_encoded, new_cuisine_df], axis=1)\n",
        "\n",
        "    # Align columns with the restaurant features (df_template)\n",
        "    # This is crucial: add missing columns from template and fill with 0\n",
        "    # Drop extra columns that are not in the template\n",
        "    user_profile_aligned = user_profile_df.reindex(columns=df_template.columns, fill_value=0)\n",
        "\n",
        "    # 4. Feature Scaling for numerical columns\n",
        "    # Ensure numerical_cols_to_scale only contains columns present in user_profile_aligned\n",
        "    cols_to_scale_in_user_profile = [col for col in num_cols_to_scale if col in user_profile_aligned.columns]\n",
        "    user_profile_aligned[cols_to_scale_in_user_profile] = scaler_obj.transform(user_profile_aligned[cols_to_scale_in_user_profile])\n",
        "\n",
        "    return user_profile_aligned.iloc[0] # Return as a Series\n",
        "\n",
        "\n",
        "def get_recommendations(user_preferences, df_restaurants_features, scaler_obj, ohe_cat_cols, bin_cols, all_cuisines_list, num_cols_to_scale, top_n=10):\n",
        "    \"\"\"\n",
        "    Recommends restaurants based on user preferences using content-based filtering.\n",
        "\n",
        "    Args:\n",
        "        user_preferences (dict): Dictionary of user preferences.\n",
        "                                 Can include 'Min_Rating' for filtering.\n",
        "        df_restaurants_features (pd.DataFrame): The preprocessed DataFrame of all restaurants\n",
        "                                                with features, ID, Name, and Aggregate rating.\n",
        "        scaler_obj (StandardScaler): The fitted scaler used for numerical features.\n",
        "        ohe_cat_cols (list): List of original categorical columns for one-hot encoding.\n",
        "        bin_cols (list): List of original binary columns for 0/1 conversion.\n",
        "        all_cuisines_list (list): List of all unique cuisines for multi-label encoding.\n",
        "        num_cols_to_scale (list): List of numerical columns that were scaled.\n",
        "        top_n (int): Number of top recommendations to return.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame of top N recommended restaurants with their details.\n",
        "    \"\"\"\n",
        "    # 1. Filter restaurants based on explicit user criteria (e.g., minimum rating)\n",
        "    filtered_df = df_restaurants_features.copy()\n",
        "    user_preferences_for_profile = user_preferences.copy() # Create a copy to modify for profile creation\n",
        "\n",
        "    if 'Min_Rating' in user_preferences_for_profile:\n",
        "        min_rating = user_preferences_for_profile['Min_Rating']\n",
        "        filtered_df = filtered_df[filtered_df['Aggregate rating'] >= min_rating]\n",
        "        print(f\"Filtered for restaurants with Min_Rating >= {min_rating}. Remaining: {len(filtered_df)} restaurants.\")\n",
        "        # Remove Min_Rating from user_preferences_for_profile as it's a filter, not a feature\n",
        "        del user_preferences_for_profile['Min_Rating']\n",
        "\n",
        "\n",
        "    # If City is a hard filter, apply it before similarity\n",
        "    if 'City' in user_preferences_for_profile:\n",
        "        city_pref = user_preferences_for_profile['City']\n",
        "        # Find the one-hot encoded city column name\n",
        "        city_col_name = f'City_{city_pref}'\n",
        "        if city_col_name in filtered_df.columns:\n",
        "            filtered_df = filtered_df[filtered_df[city_col_name] == 1].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
        "            print(f\"Filtered for City: {city_pref}. Remaining: {len(filtered_df)} restaurants.\")\n",
        "        else:\n",
        "             # Handle cases where the city preference doesn't exist in the dataset after filtering\n",
        "             print(f\"Warning: City '{city_pref}' not found in the dataset after other filters. No restaurants remaining.\")\n",
        "             return pd.DataFrame(columns=['Restaurant Name', 'Aggregate rating', 'Similarity Score'])\n",
        "\n",
        "        # Remove City from user_preferences_for_profile as it's already filtered\n",
        "        del user_preferences_for_profile['City']\n",
        "\n",
        "    # Add filtering for 'Votes' if present in user preferences\n",
        "    if 'Votes' in user_preferences_for_profile:\n",
        "        min_votes = user_preferences_for_profile['Votes']\n",
        "        filtered_df = filtered_df[filtered_df['Votes'] >= min_votes].copy() # Use .copy()\n",
        "        print(f\"Filtered for restaurants with at least {min_votes} votes. Remaining: {len(filtered_df)} restaurants.\")\n",
        "        # Remove Votes from user_preferences_for_profile as it's a filter\n",
        "        del user_preferences_for_profile['Votes']\n",
        "\n",
        "\n",
        "    # Ensure there are restaurants left after filtering\n",
        "    if filtered_df.empty:\n",
        "        print(\"No restaurants found matching the filtering criteria.\")\n",
        "        return pd.DataFrame(columns=['Restaurant Name', 'Aggregate rating', 'Similarity Score'])\n",
        "\n",
        "    # 2. Create user profile vector\n",
        "    # Get the feature columns from the filtered_df (excluding ID, Name, Rating)\n",
        "    restaurant_feature_cols = [col for col in filtered_df.columns if col not in ['Restaurant ID', 'Restaurant Name', 'Aggregate rating', 'Similarity Score']]\n",
        "\n",
        "\n",
        "    # Pass a template DataFrame for user profile creation that matches feature columns\n",
        "    user_profile_vector = create_user_profile(\n",
        "        user_preferences_for_profile,\n",
        "        filtered_df[restaurant_feature_cols], # Use filtered_df's feature columns as template\n",
        "        scaler_obj,\n",
        "        ohe_cat_cols,\n",
        "        bin_cols,\n",
        "        all_cuisines_list,\n",
        "        num_cols_to_scale\n",
        "    )\n",
        "\n",
        "    # Convert user profile to a 2D array for cosine_similarity\n",
        "    user_profile_array = user_profile_vector.values.reshape(1, -1)\n",
        "\n",
        "    # Extract features from filtered restaurants for similarity calculation\n",
        "    restaurant_features_array = filtered_df[restaurant_feature_cols].values\n",
        "\n",
        "    # 3. Calculate Cosine Similarity\n",
        "    # Handle cases where user profile might have all zeros (e.g., no preferences matching features)\n",
        "    if np.linalg.norm(user_profile_array) == 0:\n",
        "        print(\"User profile is all zeros. Cannot calculate meaningful similarity.\")\n",
        "        return pd.DataFrame(columns=['Restaurant Name', 'Aggregate rating', 'Similarity Score'])\n",
        "\n",
        "    similarity_scores = cosine_similarity(user_profile_array, restaurant_features_array).flatten()\n",
        "\n",
        "    # 4. Add similarity scores to the filtered DataFrame\n",
        "    filtered_df['Similarity Score'] = similarity_scores\n",
        "\n",
        "    # 5. Sort by similarity score in descending order\n",
        "    recommended_restaurants = filtered_df.sort_values(by='Similarity Score', ascending=False)\n",
        "\n",
        "    # 6. Return top N recommendations\n",
        "    return recommended_restaurants[['Restaurant Name', 'Aggregate rating', 'Similarity Score']].head(top_n)\n",
        "\n",
        "print(\"Recommendation system core logic loaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KqED0RpSnZ1",
        "outputId": "6c0a8d1f-14e4-4d5c-cb91-09fff3f582d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- CELL 2: Recommendation System Core Logic ---\n",
            "Recommendation system core logic loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd # Ensure pandas is imported if running this cell independently\n",
        "# Assuming df_processed_for_recommender, scaler_recommender, etc. are global from Cell 1 and 2\n",
        "\n",
        "print(\"--- CELL 3: Test the Recommendation System ---\")\n",
        "\n",
        "# Define sample user preferences\n",
        "user_pref_1 = {\n",
        "    'City': 'New Delhi',\n",
        "    'Cuisines': ['Italian', 'Pizza'],\n",
        "    'Price range': 2,\n",
        "    'Has Online delivery': 'Yes',\n",
        "    'Min_Rating': 3.8, # User wants highly-rated restaurants\n",
        "    'Votes': 100 # User prefers restaurants with at least some votes\n",
        "}\n",
        "\n",
        "user_pref_2 = {\n",
        "    'City': 'London',\n",
        "    'Cuisines': ['Cafe', 'Desserts', 'European'],\n",
        "    'Average Cost for two': 50, # USD for London\n",
        "    'Currency': 'Pounds(GBP)',\n",
        "    'Has Table booking': 'Yes',\n",
        "    'Min_Rating': 4.0\n",
        "}\n",
        "\n",
        "user_pref_3 = {\n",
        "    'City': 'Bangalore',\n",
        "    'Cuisines': ['North Indian', 'Biryani'],\n",
        "    'Price range': 3,\n",
        "    'Has Online delivery': 'No', # User prefers no online delivery\n",
        "    'Min_Rating': 3.0\n",
        "}\n",
        "\n",
        "print(\"\\n--- Testing with User Preferences 1 (New Delhi, Italian/Pizza, High Rating) ---\")\n",
        "recommendations_1 = get_recommendations(\n",
        "    user_pref_1,\n",
        "    df_processed_for_recommender,\n",
        "    scaler_recommender,\n",
        "    original_categorical_cols_recommender,\n",
        "    original_binary_cols_recommender,\n",
        "    original_all_cuisines_recommender,\n",
        "    numerical_cols_to_scale # Pass numerical_cols_to_scale\n",
        ")\n",
        "if not recommendations_1.empty:\n",
        "    print(recommendations_1.to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
        "else:\n",
        "    print(\"No recommendations found for this preference.\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Testing with User Preferences 2 (London, Cafe/Desserts, Very High Rating) ---\")\n",
        "recommendations_2 = get_recommendations(\n",
        "    user_pref_2,\n",
        "    df_processed_for_recommender,\n",
        "    scaler_recommender,\n",
        "    original_categorical_cols_recommender,\n",
        "    original_binary_cols_recommender,\n",
        "    original_all_cuisines_recommender,\n",
        "    numerical_cols_to_scale # Pass numerical_cols_to_scale\n",
        ")\n",
        "if not recommendations_2.empty:\n",
        "    print(recommendations_2.to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
        "else:\n",
        "    print(\"No recommendations found for this preference.\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Testing with User Preferences 3 (Bangalore, North Indian/Biryani, Moderate Rating, No Online Delivery) ---\")\n",
        "recommendations_3 = get_recommendations(\n",
        "    user_pref_3,\n",
        "    df_processed_for_recommender,\n",
        "    scaler_recommender,\n",
        "    original_categorical_cols_recommender,\n",
        "    original_binary_cols_recommender,\n",
        "    original_all_cuisines_recommender,\n",
        "    numerical_cols_to_scale # Pass numerical_cols_to_scale\n",
        ")\n",
        "if not recommendations_3.empty:\n",
        "    print(recommendations_3.to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
        "else:\n",
        "    print(\"No recommendations found for this preference.\")\n",
        "\n",
        "\n",
        "print(\"\\nRecommendation system testing complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1KdWK0eStSn",
        "outputId": "1174a5e4-40e9-4782-cc69-a37f3eed78f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- CELL 3: Test the Recommendation System ---\n",
            "\n",
            "--- Testing with User Preferences 1 (New Delhi, Italian/Pizza, High Rating) ---\n",
            "Filtered for restaurants with Min_Rating >= 3.8. Remaining: 2109 restaurants.\n",
            "Filtered for City: New Delhi. Remaining: 688 restaurants.\n",
            "Filtered for restaurants with at least 100 votes. Remaining: 0 restaurants.\n",
            "No restaurants found matching the filtering criteria.\n",
            "No recommendations found for this preference.\n",
            "\n",
            "--- Testing with User Preferences 2 (London, Cafe/Desserts, Very High Rating) ---\n",
            "Filtered for restaurants with Min_Rating >= 4.0. Remaining: 1378 restaurants.\n",
            "Filtered for City: London. Remaining: 19 restaurants.\n",
            "| Restaurant Name   | Aggregate rating   | Similarity Score   |\n",
            "|:------------------|:-------------------|:-------------------|\n",
            "| Masala Zone       | 4.1                | 0.184763           |\n",
            "| Roti Chai         | 4.5                | 0.0777129          |\n",
            "| Gymkhana          | 4.7                | -0.0127951         |\n",
            "| Nobu              | 4.4                | -0.0163677         |\n",
            "| Hakkasan          | 4.8                | -0.0192235         |\n",
            "| Yauatcha          | 4.7                | -0.0456555         |\n",
            "| Dishoom           | 4.7                | -0.18313           |\n",
            "| Bao               | 4.9                | -0.235888          |\n",
            "| Shake Shack       | 4.1                | -0.246173          |\n",
            "| Flat Iron         | 4.9                | -0.251405          |\n",
            "\n",
            "--- Testing with User Preferences 3 (Bangalore, North Indian/Biryani, Moderate Rating, No Online Delivery) ---\n",
            "Filtered for restaurants with Min_Rating >= 3.0. Remaining: 5961 restaurants.\n",
            "Filtered for City: Bangalore. Remaining: 20 restaurants.\n",
            "| Restaurant Name     | Aggregate rating   | Similarity Score   |\n",
            "|:--------------------|:-------------------|:-------------------|\n",
            "| Hoot                | 3.9                | 0.499116           |\n",
            "| Communiti           | 4.2                | 0.469613           |\n",
            "| Farzi Cafe          | 4.4                | 0.384459           |\n",
            "| ECHOES Koramangala  | 4.7                | 0.351692           |\n",
            "| Bombay Brasserie    | 4.2                | 0.286158           |\n",
            "| Koramangala Social  | 4.5                | 0.262136           |\n",
            "| Onesta              | 4.6                | 0.259519           |\n",
            "| Onesta              | 4.6                | 0.235908           |\n",
            "| Flechazo            | 4.4                | 0.234159           |\n",
            "| Three Dots & A Dash | 3.9                | 0.138881           |\n",
            "\n",
            "Recommendation system testing complete.\n"
          ]
        }
      ]
    }
  ]
}